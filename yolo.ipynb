{"cells":[{"cell_type":"code","execution_count":1,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":7,"status":"ok","timestamp":1682055601615,"user":{"displayName":"Shubhankar Shandilya","userId":"01327132389198292390"},"user_tz":-330},"id":"AXY17OrTS-lP","outputId":"4fbb5f44-4c16-4948-d694-43816e3aa7d7"},"outputs":[{"name":"stdout","output_type":"stream","text":["Tue Mar 12 21:01:36 2024       \n","+-----------------------------------------------------------------------------------------+\n","| NVIDIA-SMI 551.76                 Driver Version: 551.76         CUDA Version: 12.4     |\n","|-----------------------------------------+------------------------+----------------------+\n","| GPU  Name                     TCC/WDDM  | Bus-Id          Disp.A | Volatile Uncorr. ECC |\n","| Fan  Temp   Perf          Pwr:Usage/Cap |           Memory-Usage | GPU-Util  Compute M. |\n","|                                         |                        |               MIG M. |\n","|=========================================+========================+======================|\n","|   0  NVIDIA GeForce GTX 1650      WDDM  |   00000000:01:00.0  On |                  N/A |\n","| N/A   51C    P0             14W /   50W |     792MiB /   4096MiB |     15%      Default |\n","|                                         |                        |                  N/A |\n","+-----------------------------------------+------------------------+----------------------+\n","                                                                                         \n","+-----------------------------------------------------------------------------------------+\n","| Processes:                                                                              |\n","|  GPU   GI   CI        PID   Type   Process name                              GPU Memory |\n","|        ID   ID                                                               Usage      |\n","|=========================================================================================|\n","|    0   N/A  N/A       600    C+G   ...8.0_x64__cv1g1gvanyjgm\\WhatsApp.exe      N/A      |\n","|    0   N/A  N/A      3816    C+G   ...oogle\\Chrome\\Application\\chrome.exe      N/A      |\n","|    0   N/A  N/A      5656    C+G   ...t.LockApp_cw5n1h2txyewy\\LockApp.exe      N/A      |\n","|    0   N/A  N/A      6940    C+G   ...Programs\\Microsoft VS Code\\Code.exe      N/A      |\n","|    0   N/A  N/A      7888    C+G   ...5n1h2txyewy\\ShellExperienceHost.exe      N/A      |\n","|    0   N/A  N/A      8020    C+G   C:\\Windows\\explorer.exe                     N/A      |\n","|    0   N/A  N/A      8380    C+G   ....Search_cw5n1h2txyewy\\SearchApp.exe      N/A      |\n","|    0   N/A  N/A     10832    C+G   ...GeForce Experience\\NVIDIA Share.exe      N/A      |\n","|    0   N/A  N/A     11752    C+G   ...CBS_cw5n1h2txyewy\\TextInputHost.exe      N/A      |\n","|    0   N/A  N/A     11808    C+G   ...crosoft\\Edge\\Application\\msedge.exe      N/A      |\n","|    0   N/A  N/A     12440    C+G   ...egxr34yet59cg\\Package\\QuickLook.exe      N/A      |\n","|    0   N/A  N/A     13532    C+G   ...siveControlPanel\\SystemSettings.exe      N/A      |\n","+-----------------------------------------------------------------------------------------+\n"]}],"source":["!nvidia-smi\n"]},{"cell_type":"code","execution_count":2,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":17553,"status":"ok","timestamp":1682075120658,"user":{"displayName":"Shubhankar Shandilya","userId":"01327132389198292390"},"user_tz":-330},"id":"PQ6RkiuB8rvS","outputId":"085d8dd4-d865-4832-84bb-f272fac4b6fb"},"outputs":[{"name":"stderr","output_type":"stream","text":["Ultralytics YOLOv8.0.20  Python-3.10.6 torch-2.2.1+cpu CPU\n","Setup complete  (8 CPUs, 15.9 GB RAM, 171.4/476.3 GB disk)\n"]}],"source":["!pip install ultralytics==8.0.20\n","\n","from IPython import display\n","display.clear_output()\n","\n","import ultralytics\n","ultralytics.checks()"]},{"cell_type":"code","execution_count":3,"metadata":{"executionInfo":{"elapsed":518,"status":"ok","timestamp":1682075156174,"user":{"displayName":"Shubhankar Shandilya","userId":"01327132389198292390"},"user_tz":-330},"id":"BbzJv2ye81Ov"},"outputs":[],"source":["from ultralytics import YOLO\n","\n","from IPython.display import display, Image"]},{"cell_type":"code","execution_count":5,"metadata":{},"outputs":[{"data":{"text/plain":["'c:\\\\Users\\\\Theeraj\\\\Desktop\\\\Cyber Security\\\\Accident Detection Model\\\\Accident-Detection-Model'"]},"execution_count":5,"metadata":{},"output_type":"execute_result"}],"source":["pwd"]},{"cell_type":"code","execution_count":4,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":114827,"status":"ok","timestamp":1682075330434,"user":{"displayName":"Shubhankar Shandilya","userId":"01327132389198292390"},"user_tz":-330},"id":"onGbGuMvMMVz","outputId":"af6d921e-7652-482d-ac13-e02513327721"},"outputs":[{"name":"stdout","output_type":"stream","text":["/content/drive/MyDrive/Accident Detection model\n","Ultralytics YOLOv8.0.20 ðŸš€ Python-3.9.16 torch-2.0.0+cu118 CUDA:0 (Tesla T4, 15102MiB)\n","\u001b[34m\u001b[1myolo/engine/trainer: \u001b[0mtask=detect, mode=train, model=yolov8s.yaml, data=data.yaml, epochs=1, patience=50, batch=16, imgsz=640, save=True, cache=False, device=, workers=8, project=None, name=None, exist_ok=False, pretrained=False, optimizer=SGD, verbose=True, seed=0, deterministic=True, single_cls=False, image_weights=False, rect=False, cos_lr=False, close_mosaic=10, resume=False, overlap_mask=True, mask_ratio=4, dropout=False, val=True, save_json=False, save_hybrid=False, conf=0.001, iou=0.7, max_det=300, half=False, dnn=False, plots=True, source=ultralytics/assets/, show=False, save_txt=False, save_conf=False, save_crop=False, hide_labels=False, hide_conf=False, vid_stride=1, line_thickness=3, visualize=False, augment=False, agnostic_nms=False, classes=None, retina_masks=False, boxes=True, format=torchscript, keras=False, optimize=False, int8=False, dynamic=False, simplify=False, opset=17, workspace=4, nms=False, lr0=0.01, lrf=0.01, momentum=0.937, weight_decay=0.001, warmup_epochs=3.0, warmup_momentum=0.8, warmup_bias_lr=0.1, box=7.5, cls=0.5, dfl=1.5, fl_gamma=0.0, label_smoothing=0.0, nbs=64, hsv_h=0.015, hsv_s=0.7, hsv_v=0.4, degrees=0.0, translate=0.1, scale=0.5, shear=0.0, perspective=0.0, flipud=0.0, fliplr=0.5, mosaic=1.0, mixup=0.0, copy_paste=0.0, cfg=None, v5loader=False, save_dir=runs/detect/train3\n","Downloading https://ultralytics.com/assets/Arial.ttf to /root/.config/Ultralytics/Arial.ttf...\n","100% 755k/755k [00:00<00:00, 17.4MB/s]\n","2023-04-21 11:07:00.647591: I tensorflow/core/platform/cpu_feature_guard.cc:182] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n","To enable the following instructions: AVX2 FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n","2023-04-21 11:07:01.631778: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT\n","Overriding model.yaml nc=80 with nc=1\n","\n","                   from  n    params  module                                       arguments                     \n","  0                  -1  1       928  ultralytics.nn.modules.Conv                  [3, 32, 3, 2]                 \n","  1                  -1  1     18560  ultralytics.nn.modules.Conv                  [32, 64, 3, 2]                \n","  2                  -1  1     29056  ultralytics.nn.modules.C2f                   [64, 64, 1, True]             \n","  3                  -1  1     73984  ultralytics.nn.modules.Conv                  [64, 128, 3, 2]               \n","  4                  -1  2    197632  ultralytics.nn.modules.C2f                   [128, 128, 2, True]           \n","  5                  -1  1    295424  ultralytics.nn.modules.Conv                  [128, 256, 3, 2]              \n","  6                  -1  2    788480  ultralytics.nn.modules.C2f                   [256, 256, 2, True]           \n","  7                  -1  1   1180672  ultralytics.nn.modules.Conv                  [256, 512, 3, 2]              \n","  8                  -1  1   1838080  ultralytics.nn.modules.C2f                   [512, 512, 1, True]           \n","  9                  -1  1    656896  ultralytics.nn.modules.SPPF                  [512, 512, 5]                 \n"," 10                  -1  1         0  torch.nn.modules.upsampling.Upsample         [None, 2, 'nearest']          \n"," 11             [-1, 6]  1         0  ultralytics.nn.modules.Concat                [1]                           \n"," 12                  -1  1    591360  ultralytics.nn.modules.C2f                   [768, 256, 1]                 \n"," 13                  -1  1         0  torch.nn.modules.upsampling.Upsample         [None, 2, 'nearest']          \n"," 14             [-1, 4]  1         0  ultralytics.nn.modules.Concat                [1]                           \n"," 15                  -1  1    148224  ultralytics.nn.modules.C2f                   [384, 128, 1]                 \n"," 16                  -1  1    147712  ultralytics.nn.modules.Conv                  [128, 128, 3, 2]              \n"," 17            [-1, 12]  1         0  ultralytics.nn.modules.Concat                [1]                           \n"," 18                  -1  1    493056  ultralytics.nn.modules.C2f                   [384, 256, 1]                 \n"," 19                  -1  1    590336  ultralytics.nn.modules.Conv                  [256, 256, 3, 2]              \n"," 20             [-1, 9]  1         0  ultralytics.nn.modules.Concat                [1]                           \n"," 21                  -1  1   1969152  ultralytics.nn.modules.C2f                   [768, 512, 1]                 \n"," 22        [15, 18, 21]  1   2116435  ultralytics.nn.modules.Detect                [1, [128, 256, 512]]          \n","Model summary: 225 layers, 11135987 parameters, 11135971 gradients, 28.6 GFLOPs\n","\n","Transferred 349/355 items from pretrained weights\n","\u001b[34m\u001b[1moptimizer:\u001b[0m SGD(lr=0.01) with parameter groups 57 weight(decay=0.0), 64 weight(decay=0.001), 63 bias\n","\u001b[34m\u001b[1mtrain: \u001b[0mScanning /content/drive/MyDrive/Accident Detection model/data/train/labels.cache... 947 images, 446 backgrounds, 0 corrupt: 100% 947/947 [00:00<?, ?it/s]\n","\u001b[34m\u001b[1malbumentations: \u001b[0mBlur(p=0.01, blur_limit=(3, 7)), MedianBlur(p=0.01, blur_limit=(3, 7)), ToGray(p=0.01), CLAHE(p=0.01, clip_limit=(1, 4.0), tile_grid_size=(8, 8))\n","\u001b[34m\u001b[1mval: \u001b[0mScanning /content/drive/MyDrive/Accident Detection model/data/valid/labels.cache... 154 images, 18 backgrounds, 0 corrupt: 100% 154/154 [00:00<?, ?it/s]\n","Image sizes 640 train, 640 val\n","Using 2 dataloader workers\n","Logging results to \u001b[1mruns/detect/train3\u001b[0m\n","Starting training for 1 epochs...\n","\n","      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n","        1/1      3.94G       2.22      5.472      2.093          1        640: 100% 60/60 [00:55<00:00,  1.08it/s]\n","                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100% 5/5 [00:18<00:00,  3.77s/it]\n","                   all        154        170      0.534      0.406      0.429      0.201\n","\n","1 epochs completed in 0.022 hours.\n","Optimizer stripped from runs/detect/train3/weights/last.pt, 22.5MB\n","Optimizer stripped from runs/detect/train3/weights/best.pt, 22.5MB\n","\n","Validating runs/detect/train3/weights/best.pt...\n","Ultralytics YOLOv8.0.20 ðŸš€ Python-3.9.16 torch-2.0.0+cu118 CUDA:0 (Tesla T4, 15102MiB)\n","Model summary (fused): 168 layers, 11125971 parameters, 0 gradients, 28.4 GFLOPs\n","                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100% 5/5 [00:06<00:00,  1.38s/it]\n","                   all        154        170      0.535        0.4      0.429      0.201\n","Speed: 5.5ms pre-process, 5.0ms inference, 0.0ms loss, 5.3ms post-process per image\n","Results saved to \u001b[1mruns/detect/train3\u001b[0m\n"]}],"source":["!yolo task=detect mode=train model=r'/run/detec/train/weights/best.pt' data= data.yaml epochs=1 imgsz=640 plots=True"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":19932,"status":"ok","timestamp":1682013490562,"user":{"displayName":"Shubhankar Shandilya","userId":"01327132389198292390"},"user_tz":-330},"id":"Q1Bp-ldxFaLf","outputId":"55d09109-8349-49d9-88b6-d81652839795"},"outputs":[{"name":"stdout","output_type":"stream","text":["2023-04-20 17:57:54.973371: I tensorflow/core/platform/cpu_feature_guard.cc:182] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n","To enable the following instructions: AVX2 AVX512F FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n","2023-04-20 17:57:55.849246: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT\n","Ultralytics YOLOv8.0.20 ðŸš€ Python-3.9.16 torch-2.0.0+cu118 CUDA:0 (Tesla T4, 15102MiB)\n","Model summary (fused): 168 layers, 11125971 parameters, 0 gradients, 28.4 GFLOPs\n","\u001b[34m\u001b[1mval: \u001b[0mScanning /content/drive/MyDrive/Accident Detection model/data/valid/labels.cache... 154 images, 18 backgrounds, 0 corrupt: 100% 154/154 [00:00<?, ?it/s]\n","                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100% 10/10 [00:07<00:00,  1.42it/s]\n","                   all        154        170      0.586      0.524      0.592      0.275\n","Speed: 1.8ms pre-process, 13.7ms inference, 0.0ms loss, 5.1ms post-process per image\n"]}],"source":["!yolo task=detect mode=val model=runs/detect/train/weights/best.pt data=data.yaml"]},{"cell_type":"code","execution_count":6,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":13936,"status":"ok","timestamp":1682013520766,"user":{"displayName":"Shubhankar Shandilya","userId":"01327132389198292390"},"user_tz":-330},"id":"2GyRu-6NJRPU","outputId":"1be25396-fc64-4753-d3a8-40c42d7aa60c"},"outputs":[],"source":["!yolo task=detect mode=predict model=runs/detect/train/weights/best.pt conf=0.25 source=data/test/images"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":12217,"status":"ok","timestamp":1682069039133,"user":{"displayName":"Shubhankar Shandilya","userId":"01327132389198292390"},"user_tz":-330},"id":"JbeWz4MlL7qB","outputId":"55a1cf3b-61a2-4fa3-c1cc-5d81866824a7"},"outputs":[{"name":"stdout","output_type":"stream","text":["2023-04-21 09:23:50.634575: I tensorflow/core/platform/cpu_feature_guard.cc:182] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n","To enable the following instructions: AVX2 AVX512F FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n","2023-04-21 09:23:51.465151: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT\n","Ultralytics YOLOv8.0.20 ðŸš€ Python-3.9.16 torch-2.0.0+cu118 CUDA:0 (Tesla T4, 15102MiB)\n","Model summary (fused): 168 layers, 11125971 parameters, 0 gradients, 28.4 GFLOPs\n","image 1/1 /content/drive/MyDrive/Accident Detection model/data/testing1.jpg: 640x640 1 Accident, 16.3ms\n","Speed: 0.9ms pre-process, 16.3ms inference, 114.2ms postprocess per image at shape (1, 3, 640, 640)\n"]}],"source":["!yolo task=detect mode=predict model=runs/detect/train/weights/best.pt source=\"/content/drive/MyDrive/Accident Detection model/data/testing1.jpg\""]},{"cell_type":"code","execution_count":6,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":3772,"status":"ok","timestamp":1682075461476,"user":{"displayName":"Shubhankar Shandilya","userId":"01327132389198292390"},"user_tz":-330},"id":"pTHGkHaLQqp8","outputId":"cc740bb6-5b4f-4fe0-ba8c-127f05980019"},"outputs":[{"name":"stdout","output_type":"stream","text":["Traceback (most recent call last):\n","  File \"/usr/local/bin/yolo\", line 8, in <module>\n","    sys.exit(entrypoint())\n","  File \"/usr/local/lib/python3.9/dist-packages/ultralytics/yolo/cfg/__init__.py\", line 212, in entrypoint\n","    raise argument_error(a)\n","SyntaxError: '-thresh' is not a valid YOLO argument.\n","\n","    YOLOv8 'yolo' CLI commands use the following syntax:\n","\n","        yolo TASK MODE ARGS\n","\n","        Where   TASK (optional) is one of [detect, segment, classify]\n","                MODE (required) is one of [train, val, predict, export]\n","                ARGS (optional) are any number of custom 'arg=value' pairs like 'imgsz=320' that override defaults.\n","                    See all ARGS at https://docs.ultralytics.com/cfg or with 'yolo cfg'\n","\n","    1. Train a detection model for 10 epochs with an initial learning_rate of 0.01\n","        yolo detect train data=coco128.yaml model=yolov8n.pt epochs=10 lr0=0.01\n","\n","    2. Predict a YouTube video using a pretrained segmentation model at image size 320:\n","        yolo segment predict model=yolov8n-seg.pt source='https://youtu.be/Zgi9g1ksQHc' imgsz=320\n","\n","    3. Val a pretrained detection model at batch-size 1 and image size 640:\n","        yolo detect val model=yolov8n.pt data=coco128.yaml batch=1 imgsz=640\n","\n","    4. Export a YOLOv8n classification model to ONNX format at image size 224 by 128 (no TASK required)\n","        yolo export model=yolov8n-cls.pt format=onnx imgsz=224,128\n","\n","    5. Run special commands:\n","        yolo help\n","        yolo checks\n","        yolo version\n","        yolo settings\n","        yolo copy-cfg\n","        yolo cfg\n","\n","    Docs: https://docs.ultralytics.com/cli\n","    Community: https://community.ultralytics.com\n","    GitHub: https://github.com/ultralytics/ultralytics\n","    \n","No accident detected\n"]}],"source":["!yolo task=detect mode=predict model=runs/detect/train/weights/best.pt source=\"/content/drive/MyDrive/Accident Detection model/data/testing1.jpg\" -thresh 0.01 | grep \"ACCIDENT\" && echo \"Accident is present\" || echo \"No accident detected\"\n"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":374},"executionInfo":{"elapsed":1996,"status":"error","timestamp":1682069746131,"user":{"displayName":"Shubhankar Shandilya","userId":"01327132389198292390"},"user_tz":-330},"id":"g8ugQqunQ8m7","outputId":"1aa393b3-c509-4f85-8b65-9111cc470a66"},"outputs":[{"ename":"error","evalue":"ignored","output_type":"error","traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31merror\u001b[0m                                     Traceback (most recent call last)","\u001b[0;32m<ipython-input-13-474ad9d4c1da>\u001b[0m in \u001b[0;36m<cell line: 49>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     47\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     48\u001b[0m \u001b[0;31m# Get the number of objects detected\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 49\u001b[0;31m \u001b[0mnum_objects\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdetect_objects\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0moutput_image\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmodel_path\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     50\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     51\u001b[0m \u001b[0;31m# Set a threshold for the number of objects\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m<ipython-input-13-474ad9d4c1da>\u001b[0m in \u001b[0;36mdetect_objects\u001b[0;34m(image, model_path, conf_thres)\u001b[0m\n\u001b[1;32m      6\u001b[0m \u001b[0;32mdef\u001b[0m \u001b[0mdetect_objects\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mimage\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmodel_path\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mconf_thres\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m0.4\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      7\u001b[0m     \u001b[0;31m# Load the model\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 8\u001b[0;31m     \u001b[0mnet\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcv2\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdnn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreadNet\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel_path\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      9\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     10\u001b[0m     \u001b[0;31m# Get the names of the output layers\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;31merror\u001b[0m: OpenCV(4.7.0) /io/opencv/modules/dnn/src/dnn_read.cpp:52: error: (-2:Unspecified error) Cannot determine an origin framework of files: /content/drive/MyDrive/Accident Detection model/runs/detect/train/weights/best.pt in function 'readNet'\n"]}],"source":["import cv2\n","\n","import torch\n","import cv2\n","\n","def detect_objects(image, model_path, conf_thres=0.4):\n","    # Load the model\n","    net = cv2.dnn.readNet(model_path)\n","\n","    # Get the names of the output layers\n","    layer_names = net.getLayerNames()\n","    output_layers = [layer_names[i[0] - 1] for i in net.getUnconnectedOutLayers()]\n","\n","    # Resize the image and normalize pixel values to [0, 1]\n","    blob = cv2.dnn.blobFromImage(image, 1/255, (416, 416), swapRB=True, crop=False)\n","\n","    # Set the input of the network\n","    net.setInput(blob)\n","\n","    # Forward pass through the network\n","    outputs = net.forward(output_layers)\n","\n","    # Parse the outputs and get the class IDs, confidences and bounding boxes\n","    class_ids, confidences, boxes = [], [], []\n","    for output in outputs:\n","        for detection in output:\n","            scores = detection[5:]\n","            class_id = np.argmax(scores)\n","            confidence = scores[class_id]\n","            if confidence > conf_thres:\n","                center_x, center_y, width, height = detection[:4] * np.array([image.shape[1], image.shape[0], image.shape[1], image.shape[0]])\n","                x, y = int(center_x - width/2), int(center_y - height/2)\n","                w, h = int(width), int(height)\n","                class_ids.append(class_id)\n","                confidences.append(float(confidence))\n","                boxes.append([x, y, w, h])\n","\n","    # Return the class IDs, confidences and bounding boxes\n","    return class_ids, confidences, boxes\n","\n","\n","# Load the output image\n","output_image = cv2.imread(\"/content/drive/MyDrive/Accident Detection model/runs/detect/predict2/testing1.jpg\")\n","\n","# Set the path to your YOLO model\n","model_path = \"/content/drive/MyDrive/Accident Detection model/runs/detect/train/weights/best.pt\"\n","\n","# Get the number of objects detected\n","num_objects = len(detect_objects(output_image, model_path))\n","\n","# Set a threshold for the number of objects\n","if num_objects > 0:\n","    message = \"Accident detected!\"\n","else:\n","    message = \"No accidents detected.\"\n"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"RkfHXDnheU3j"},"outputs":[],"source":[]}],"metadata":{"accelerator":"GPU","colab":{"authorship_tag":"ABX9TyMT2f5ozLK1kTD8LjhiD8X8","provenance":[]},"gpuClass":"standard","kernelspec":{"display_name":"Python 3","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.10.6"}},"nbformat":4,"nbformat_minor":0}
